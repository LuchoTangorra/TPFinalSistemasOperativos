{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Si quiero usar tf 2.0 lo mejor que puedo hacer es from tensorflow.keras import ... (los mismos imports). Esto es porque tf tiene mejor integracion con keras. Aunque la verdad me parece que rinde mas para proyectos que van a ser mas largos, para esto no se si rinde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uc4NJ8pista6"
   },
   "source": [
    "# Este trabajo fue realizado por Luciano Tangorra, utilizando como guia el material brindado por la cátedra de Procesamiento de lenguaje natural mediante redes neuronales, de la ECI 2019, y conocimiento previo obtenido en su universidad (UNICEN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwG8oIPZrHdp"
   },
   "source": [
    "Disculpen la desprolijidad del código y lo feo de la implementación, hice mi mejor esfuerzo.\n",
    "\n",
    "Para poder utilizar este código lo único que deben hacer es modificar las variables que poseen direction en el nombre, es decir, las direcciones de donde se obtienen los datos. Estas variables se encuentran en el código que siga a cada sección llamada preprocesamiento.\n",
    "\n",
    "Cualquier duda lucianotangorra@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/lucholomas/uncategorized\" target=\"_blank\">https://app.wandb.ai/lucholomas/uncategorized</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/lucholomas/uncategorized/runs/nqhpwxmt\" target=\"_blank\">https://app.wandb.ai/lucholomas/uncategorized/runs/nqhpwxmt</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/lucholomas/uncategorized/runs/nqhpwxmt"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just for monitoring\n",
    "import keras\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"nlp-so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "i3NJ5zS-svMn",
    "outputId": "826dd825-cda1-4ee3-9bb7-7e216dd18e77"
   },
   "outputs": [],
   "source": [
    "import pandas as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvFo6iV3s4yc"
   },
   "source": [
    "Las siguientes dos funciones son para obtener linea a linea los datos de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5x93gLas6Uy"
   },
   "outputs": [],
   "source": [
    "def read_json(sentence_data):\n",
    "    for line in sentence_data:\n",
    "        #print(line)\n",
    "        example = json.loads(line)\n",
    "        yield example['sentence2'].split()\n",
    "\n",
    "def read_cvs(tag_data, sentence_len=0):\n",
    "    for index, row in tag_data.iterrows():\n",
    "        yield row['gold_label'].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsQs5G0utSAx"
   },
   "source": [
    "Proceso los datos.\n",
    "Armo el vocabulario y le asigno un valor entero a cada indice. \n",
    "Guardo los indices de cada palabra de la sentencia como una nueva sentencia para luego utilizarla para crear los embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFxygfrStUdO"
   },
   "outputs": [],
   "source": [
    "def processData(data, tags, word_to_ix):\n",
    "    # Se encarga de procesar los datos y los tags, devolviendo un arreglo de sentencias\n",
    "    # expresadas con indices (unicos por palabra) y un arreglo de tags expresados con\n",
    "    # indices, asi como tambien la longitud maxima de sentencia\n",
    "    \n",
    "    # Params ------\n",
    "    # data: un open de un archivo json que contenga lineas con datos\n",
    "    # tag: un read_csv que contenga los tags que corresponden a cada linea de la data\n",
    "\n",
    "    # Return ------\n",
    "    # training_data: arreglo que contiene las sentencias expresadas con indices unicos por palabra\n",
    "    # training_tags: arreglo que contiene los tags para cada sentencia, expresados con indices unicos por tag\n",
    "    # max_len: maxima longitud de sentencia\n",
    "    \n",
    "    tag_to_ix = { 'entailment': 0,  'neutral': 1, 'contradiction': 2 }\n",
    "\n",
    "    #word_to_ix = {} # Dicc de palabra a indice (sin contar el 0 ya que es para el padding)\n",
    "    training_data = [] # Indices de las palabras pertenecientes a cada sentencia, por cada sentencia.\n",
    "    training_tag = [] # Indices de los tags correspondientes a cada sentencia.\n",
    "    cantidad_sentencias = 0 # Dato curioso nomas, no sirve para nada mas que para saber cuantas sentencias hay\n",
    "    max_len = 0 # maxima cantidad de palabras en una sentencia. Es para que despues todas las otras sentencias \n",
    "                # se rellenen de 0s hasta el mismo tam.\n",
    "\n",
    "    for sentence, tag in zip(read_json(data), read_cvs(tags)):\n",
    "        # Tenemos la oración en sentence con su categoría en label\n",
    "        cantidad_sentencias += 1\n",
    "        posible_max_len = 0\n",
    "        ix_sentence = []\n",
    "        for word in sentence:\n",
    "            if word[len(word)-1] == '.': # Le saco el '.' a la palabra\n",
    "                word = word[:-1]\n",
    "            posible_max_len += 1\n",
    "\n",
    "            if word not in word_to_ix:    \n",
    "                word_to_ix[word] = len(word_to_ix) + 1 # Tomo el 0 como discernible para el pad, por eso el +1\n",
    "\n",
    "            ix_sentence.append(word_to_ix[word]) # Agrego a la sentencia (hecha con indices) el indice de la palabra\n",
    "\n",
    "            if posible_max_len > max_len:\n",
    "                max_len = posible_max_len # Si el tam de la sentencia es mayor al maximo actual, lo reemplazo\n",
    "\n",
    "        training_data.append(ix_sentence)\n",
    "        training_tag.append(tag_to_ix[tag[0]])\n",
    "    vocab_size = len(word_to_ix)\n",
    "    print('Vocabulary size: ', vocab_size)\n",
    "    print(cantidad_sentencias)\n",
    "    print(word_to_ix)\n",
    "    print(tag_to_ix)\n",
    "    return training_data, training_tag, max_len, vocab_size\n",
    "\n",
    "#vocab_size = len(word_to_ix)\n",
    "#print(cantidad_sentencias)\n",
    "#print(word_to_ix)\n",
    "#print(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDeRQBNWfPKQ"
   },
   "source": [
    "# Preprocesamiento de los datos de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlNG25f7Rk1T"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import json\n",
    "import csv\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "data_direction = 'C:\\\\Users\\\\ing_l\\\\Final SO\\\\TP NLP Keras\\\\snli_1.0_train_filtered.jsonl'\n",
    "tag_direction = 'C:\\\\Users\\\\ing_l\\\\Final SO\\\\TP NLP Keras\\\\snli_1.0_train_gold_labels.csv'\n",
    "\n",
    "sentence_data = open(data_direction, 'r')\n",
    "tag_data = p.read_csv(tag_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "hP1olwXnQBmn",
    "outputId": "04c35d9a-b9e1-402c-efb9-bb423d3aef82"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cfb0ca0601af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-58e980ed3a58>\u001b[0m in \u001b[0;36mprocessData\u001b[1;34m(data, tags, word_to_ix)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[1;31m# se rellenen de 0s hasta el mismo tam.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_cvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Tenemos la oración en sentence con su categoría en label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mcantidad_sentencias\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f0feec158ddf>\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(sentence_data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;31m#print(line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\new_environment\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data, training_tag, max_len, vocab_size = processData(sentence_data, tag_data, word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iTH8T40OhLk"
   },
   "source": [
    "Keras necesita que le pase el resultado de cada neurona de salida, con un 1 donde se encuentra la salida correcta y 0s en los demas lados, el to_categorical hace exactamente eso si le paso una lista de tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vB_NVAEhCcbq"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "training_tag = to_categorical(training_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFVSz3Qehy7w"
   },
   "source": [
    "Agrego 0s al final de cada sentencia para que todas queden del mismo tamaño (Keras pide que sea asi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCZYWrmbfzx2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "training_data = pad_sequences(training_data,maxlen=max_len,padding='post',value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "R1RSC4T3TAEl",
    "outputId": "2a509239-3ef9-4d86-9410-53aee97ee452"
   },
   "outputs": [],
   "source": [
    "print(training_data.shape)\n",
    "print(training_tag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_REycPrecXlF"
   },
   "source": [
    "# Preprocesamiento de los datos de dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLCkqOUOjDY9"
   },
   "source": [
    "Lo mismo que con los datos de training. \n",
    "Se procesan los datos de las sentencias y los tags.\n",
    "A los tags se los arma como una matriz con 0s excepto en el lugar que corresponda.\n",
    "A las sentencias se las deja con 0s al final para que todas tengan la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQ4Bx4keUrdc"
   },
   "outputs": [],
   "source": [
    "dev_data_direction = 'C:\\\\Users\\\\ing_l\\\\Final SO\\\\TP NLP Keras\\\\snli_1.0_dev_filtered.jsonl'\n",
    "dev_tag_direction = 'C:\\\\Users\\\\ing_l\\\\Final SO\\\\TP NLP Keras\\\\snli_1.0_dev_gold_labels.csv'\n",
    "sentence_dev_data = open(dev_data_direction, 'r')\n",
    "tag_dev_data = p.read_csv(dev_tag_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "1KrRiSpyRV_L",
    "outputId": "530dbf11-27c7-48b1-a51f-328f4386558a"
   },
   "outputs": [],
   "source": [
    "dev_data, dev_tag, dev_max_len, _ = processData(sentence_dev_data, tag_dev_data, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgW5WNgJVTum"
   },
   "outputs": [],
   "source": [
    "dev_tag = to_categorical(dev_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EYKSGWEVPK_"
   },
   "outputs": [],
   "source": [
    "dev_data = pad_sequences(dev_data,maxlen=dev_max_len,padding='post',value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-qs4oDntgo2"
   },
   "source": [
    "#Ahora arranco con la red neuronal.\n",
    "\n",
    "\n",
    "Input: tiene de output None, es decir, puede tener tamaño variable el input por cada batch (aunque hice el padding para que todas las sentencias tengan el mismo tam, de otro modo tendria que haber hecho bucketing para que se agrupen todas las sentencias de la misma longitud para cada batch).\n",
    "\n",
    "\n",
    "Embedding: - input_dim es el tam del vocabulario (+1 porque tomo el 0 por el padding), serian las filas de la matriz de embeddings. \n",
    "                      - output_dim es el tam del embedding (cuantas relaciones semanticas va a tener para aprender sobre la palabra).\n",
    "                      - input_length es la cantidad de palabras que tiene cada sentencia. Es none ya que puede tener tamaño de sentencia variada, aunque hice que todas las sentencias tengan la misma longitud pero POR conjunto de datos, es decir que las sentencias de entrenamiento y las de test pueden tener tamaño diferentes.\n",
    "                      - mask_zero es para que ignore a los 0s, ya que los añadí por el padding.\n",
    "\n",
    "\n",
    "LSTM: - posee 100 neuronas de salida por palabra. \n",
    "            - Return_sequences me devuelve la salida de cada palabra si es true o solo la de la última (seria como de la sentencia completa) si es false (en este caso quiero la ultima para ver dada toda la sentencia que tag le corresponde). \n",
    "            - Si bien la capa LSTM posee activacion tanh por defecto la escribi para mayor legibilidad. \n",
    "            - recurrent_dropout hace un dropout entre el output y hidden de cada palabra conectado a la siguiente.\n",
    "            - dropout hace un dropout hace un dropout de la salida, \"inhabilitando\" algunas neuronas del último output.\n",
    "\n",
    "Dense: capa oculta densa de 100 neuronas, utilizando la funcion de activacion relu.\n",
    "\n",
    "Dense: capa densa de salida de la red, tiene 3 salidas ya que son los posibles tags, es una softmax para normalizar la salida.\n",
    "\n",
    "Se utilizo nadam, que seria adam con Nesterov momentum y se mantuvieron los parametros por defecto, el cual implementa un lr inicial de 0.002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "DobMM7SfuJpx",
    "outputId": "d243dd3c-fef6-4bc8-a867-ffba6062b708"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, LSTM, Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "i = Input(shape=(None,))  \n",
    "d = Embedding(input_dim=vocab_size + 1,output_dim=128, mask_zero=True)(i)\n",
    "d = LSTM(128, activation='tanh', return_sequences=False, recurrent_dropout=0.4, dropout=0.4)(d)\n",
    "d = Dense(100, activation='relu')(d)\n",
    "d = Dense(3, activation='softmax')(d)\n",
    "\n",
    "\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "#Con adam no anduvo muy bien, con RMSprop tampoco\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rr_I3o8HphQv"
   },
   "source": [
    "Entreno la red y veo la acc del dev por cada epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "iBM2O3yfuwYB",
    "outputId": "db62e6c7-76e3-4dd3-a370-cb7188b77907"
   },
   "outputs": [],
   "source": [
    "h = model.fit(training_data, training_tag, epochs=3, batch_size=512, validation_data=(dev_data, dev_tag), callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6D0KenE8pplB"
   },
   "source": [
    "Ploteo los datos para que sea mas facil de observar la perdida y la acc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "id": "QHoWmSB2uwkv",
    "outputId": "fbe66ec3-9076-4828-f89f-c2bcd01265b4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(h.history['categorical_accuracy'])\n",
    "plt.plot(h.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkNHYdqtnhbf"
   },
   "source": [
    "# Ahora con los datos de test!\n",
    "# Preprocesamiento datos test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "ZAeOssXCeL0h",
    "outputId": "f17f2c24-7504-42a0-e07d-d0e0bf830e97"
   },
   "outputs": [],
   "source": [
    "test_data_direction = 'C:\\\\Users\\\\ing_l\\\\Final SO\\\\TP NLP Keras\\\\snli_1.0_test_filtered.jsonl'\n",
    "\n",
    "sentence_test_data = open(test_data_direction, 'r')\n",
    "\n",
    "#Dev_tag es solo para que funcione\n",
    "tag_dev_data = p.read_csv(dev_tag_direction)\n",
    "test_data, _, test_max_len, _ = processData(sentence_test_data, tag_dev_data, word_to_ix)\n",
    "\n",
    "test_data = pad_sequences(test_data,maxlen=test_max_len,padding='post',value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDAruUeonmQ4"
   },
   "source": [
    "Predigo los resultados y los convierto a una salida legible (Neutral, entailment, contradiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBhh0NRPgirr"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "cgS7G8ztibzY",
    "outputId": "fbc3b107-25db-4af1-9fad-b6f54cc69ca2"
   },
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "d4hjywzwidTO",
    "outputId": "7e8982da-6518-4fda-b6cb-75e78fe657ff"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "Kh1r1OsZkCvw",
    "outputId": "662b3e11-2245-4412-d087-5a8c8fdce157"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Yd27LxqDkvqM",
    "outputId": "98b9aea8-2612-40a1-9852-303cdc678916"
   },
   "outputs": [],
   "source": [
    "ix_to_tag = { 0: 'entailment',  1: 'neutral', 2: 'contradiction' }\n",
    "tags = np.argmax(predictions, axis=1)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Zlqh02bZlMBC",
    "outputId": "7e5fd03b-b8c2-449b-bb14-d3a7da4fa653"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for prediction in tags:\n",
    "    results.append(ix_to_tag[prediction])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oM07aTdhGo-"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def guardar(test_tags):\n",
    "    \"Junta el archivo con las oraciones de test (jsonl)\"\n",
    "    \" y los resultados de la clasificación de tu algoritmo (en tu formato)\"\n",
    "    \" en un archivo csv compatible con el formato de Kaggle\"\n",
    "\n",
    "    sentences_filename = '/content/drive/My Drive/Estudios/Cursos/ECI/snli_1.0_test_filtered.jsonl'\n",
    "    #labels_filename = \"test_cls.txt\"\n",
    "    output_filename = \"result.csv\"\n",
    "\n",
    "    with open(output_filename, 'w') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        csv_writer.writerow(['pairID', 'gold_label'])\n",
    "\n",
    "        for pairID, label in it_ID_label_pairs(sentences_filename, test_tags):\n",
    "            #formatted_label = format_label(label)\n",
    "            #print(formatted_label)\n",
    "            \n",
    "            csv_writer.writerow([pairID, label])\n",
    "\n",
    "def format_label(label):\n",
    "    return label[len(\"__label__\"):]\n",
    "\n",
    "def it_ID_label_pairs(sentences_filename, test_tags):\n",
    "    sentence_data = open(sentences_filename, 'r')\n",
    "    #labels_data = open(labels_filename, 'r')\n",
    "    for pairID, label in zip(it_ID(sentence_data), test_tags):\n",
    "        yield pairID, label\n",
    "\n",
    "def it_ID(sentence_data):\n",
    "    for line in sentence_data:\n",
    "        example = json.loads(line)\n",
    "        yield example['pairID']\n",
    "\n",
    "def it_labels(label_data):\n",
    "    for label in label_data:\n",
    "        label = label.rstrip('\\n')  # sacamos el fin de linea\n",
    "        yield label\n",
    "        \n",
    "#guardar(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP NLP Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
